{
  "_args": [
    [
      {
        "raw": "website-scraper",
        "scope": null,
        "escapedName": "website-scraper",
        "name": "website-scraper",
        "rawSpec": "",
        "spec": "latest",
        "type": "tag"
      },
      "/Users/uday-evive/Documents/sandbox/webscraper"
    ]
  ],
  "_from": "website-scraper@latest",
  "_id": "website-scraper@2.4.0",
  "_inCache": true,
  "_location": "/website-scraper",
  "_nodeVersion": "6.7.0",
  "_npmOperationalInternal": {
    "host": "packages-12-west.internal.npmjs.com",
    "tmp": "tmp/website-scraper-2.4.0.tgz_1493556298992_0.17460120376199484"
  },
  "_npmUser": {
    "name": "s0ph1e",
    "email": "sophia.nepochataya@gmail.com"
  },
  "_npmVersion": "3.10.8",
  "_phantomChildren": {},
  "_requested": {
    "raw": "website-scraper",
    "scope": null,
    "escapedName": "website-scraper",
    "name": "website-scraper",
    "rawSpec": "",
    "spec": "latest",
    "type": "tag"
  },
  "_requiredBy": [
    "#USER"
  ],
  "_resolved": "https://registry.npmjs.org/website-scraper/-/website-scraper-2.4.0.tgz",
  "_shasum": "573dbe5064d6cdad3942374c28d445b8fcb51577",
  "_shrinkwrap": null,
  "_spec": "website-scraper",
  "_where": "/Users/uday-evive/Documents/sandbox/webscraper",
  "author": {
    "name": "Sophia Antipenko",
    "email": "sophia@antipenko.pp.ua"
  },
  "bugs": {
    "url": "https://github.com/s0ph1e/node-website-scraper/issues"
  },
  "dependencies": {
    "bluebird": "^3.0.1",
    "cheerio": "0.22.0",
    "css-url-parser": "^1.0.0",
    "debug": "^2.4.5",
    "fs-extra": "^3.0.0",
    "he": "^1.1.0",
    "lodash": "^4.11.1",
    "normalize-url": "^1.5.3",
    "request": "^2.42.0",
    "srcset": "^1.0.0"
  },
  "description": "Download website to a local directory (including all css, images, js, etc.)",
  "devDependencies": {
    "codeclimate-test-reporter": "^0.4.0",
    "coveralls": "^2.11.8",
    "eslint": "^3.9.1",
    "istanbul": "^0.4.0",
    "mocha": "^3.0.2",
    "nock": "^9.0.2",
    "proxyquire": "^1.7.3",
    "should": "^11.1.0",
    "sinon": "^2.1.0"
  },
  "directories": {},
  "dist": {
    "shasum": "573dbe5064d6cdad3942374c28d445b8fcb51577",
    "tarball": "https://registry.npmjs.org/website-scraper/-/website-scraper-2.4.0.tgz"
  },
  "files": [
    "index.js",
    "lib"
  ],
  "gitHead": "2166973ff7f345b79449d4f8f2cdf61f46b68e9f",
  "homepage": "https://github.com/s0ph1e/node-website-scraper",
  "keywords": [
    "scrape",
    "scraper",
    "download",
    "web",
    "url",
    "page",
    "site",
    "html",
    "css",
    "image",
    "js"
  ],
  "license": "MIT",
  "main": "index.js",
  "maintainers": [
    {
      "name": "s0ph1e",
      "email": "sophia.nepochataya@gmail.com"
    }
  ],
  "name": "website-scraper",
  "optionalDependencies": {},
  "readme": "## Introduction\nDownload website to a local directory (including all css, images, js, etc.)\n\n[![Build Status](https://img.shields.io/travis/s0ph1e/node-website-scraper/master.svg?style=flat)](https://travis-ci.org/s0ph1e/node-website-scraper)\n[![Build status](https://ci.appveyor.com/api/projects/status/s7jxui1ngxlbgiav/branch/master?svg=true)](https://ci.appveyor.com/project/s0ph1e/node-website-scraper/branch/master)\n[![Test Coverage](https://codeclimate.com/github/s0ph1e/node-website-scraper/badges/coverage.svg)](https://codeclimate.com/github/s0ph1e/node-website-scraper/coverage)\n[![Code Climate](https://codeclimate.com/github/s0ph1e/node-website-scraper/badges/gpa.svg)](https://codeclimate.com/github/s0ph1e/node-website-scraper)\n[![Dependency Status](https://david-dm.org/s0ph1e/node-website-scraper.svg?style=flat)](https://david-dm.org/s0ph1e/node-website-scraper)\n\n[![Version](https://img.shields.io/npm/v/website-scraper.svg?style=flat)](https://www.npmjs.org/package/website-scraper)\n[![Downloads](https://img.shields.io/npm/dm/website-scraper.svg?style=flat)](https://www.npmjs.org/package/website-scraper)\n[![Gitter](https://badges.gitter.im/s0ph1e/node-website-scraper.svg)](https://gitter.im/s0ph1e/node-website-scraper?utm_source=badge&utm_medium=badge&utm_campaign=pr-badge)\n\n[![NPM Stats](https://nodei.co/npm/website-scraper.png?downloadRank=true&stars=true)](https://www.npmjs.org/package/website-scraper)\n\nYou can try it in [demo app](https://scraper.nepochataya.pp.ua/) ([source](https://github.com/s0ph1e/web-scraper))\n\n**Note:** by default dynamic websites (where content is loaded by js) may be saved not correctly because `website-scraper` doesn't execute js, it only parses http responses for html and css files. If you need to download dynamic website take a look on [website-scraper-phantom](https://github.com/s0ph1e/node-website-scraper-phantom).\n\n\n## Installation\n```\nnpm install website-scraper\n```\n\n## Usage\n```javascript\nvar scrape = require('website-scraper');\nvar options = {\n  urls: ['http://nodejs.org/'],\n  directory: '/path/to/save/',\n};\n\n// with promise\nscrape(options).then((result) => {\n\t/* some code here */\n}).catch((err) => {\n\t/* some code here */\n});\n\n// or with callback\nscrape(options, (error, result) => {\n\t/* some code here */\n});\n```\n\n## options\n* [urls](#urls) - urls to download, *required*\n* [directory](#directory) - path to save files, *required*\n* [sources](#sources) - selects which resources should be downloaded\n* [recursive](#recursive) - follow hyperlinks in html files\n* [maxRecursiveDepth](#maxrecursivedepth) - maximum depth for hyperlinks\n* [maxDepth](#maxdepth) - maximum depth for all dependencies\n* [request](#request) - custom options for for [request](https://github.com/request/request)\n* [subdirectories](#subdirectories) - subdirectories for file extensions\n* [defaultFilename](#defaultfilename) - filename for index page\n* [prettifyUrls](#prettifyurls) - prettify urls\n* [ignoreErrors](#ignoreerrors) - whether to ignore errors on resource downloading\n* [urlFilter](#urlfilter) - skip some urls\n* [filenameGenerator](#filenamegenerator) - generate filename for downloaded resource\n* [httpResponseHandler](#httpresponsehandler) - customize http response handling\n* [resourceSaver](#resourcesaver) - customize resources saving\n* [onResourceSaved](#onresourcesaved) - callback called when resource is saved\n* [onResourceError](#onresourceerror) - callback called when resource's downloading is failed\n \nDefault options you can find in [lib/config/defaults.js](https://github.com/s0ph1e/node-website-scraper/blob/master/lib/config/defaults.js) or get them using `scrape.defaults`.\n\n#### urls\nArray of objects which contain urls to download and filenames for them. **_Required_**.\n```javascript\nscrape({\n  urls: [\n    'http://nodejs.org/',\t// Will be saved with default filename 'index.html'\n    {url: 'http://nodejs.org/about', filename: 'about.html'},\n    {url: 'http://blog.nodejs.org/', filename: 'blog.html'}\n  ],\n  directory: '/path/to/save'\n}).then(console.log).catch(console.log);\n```\n\n#### directory\nString, absolute path to directory where downloaded files will be saved. Directory should not exist. It will be created by scraper. **_Required_**.\n\n#### sources\nArray of objects to download, specifies selectors and attribute values to select files for downloading. By default scraper tries to download all possible resources.\n```javascript\n// Downloading images, css files and scripts\nscrape({\n  urls: ['http://nodejs.org/'],\n  directory: '/path/to/save',\n  sources: [\n    {selector: 'img', attr: 'src'},\n    {selector: 'link[rel=\"stylesheet\"]', attr: 'href'},\n    {selector: 'script', attr: 'src'}\n  ]\n}).then(console.log).catch(console.log);\n```\n\n#### recursive\nBoolean, if `true` scraper will follow hyperlinks in html files. Don't forget to set `maxRecursiveDepth` to avoid infinite downloading. Defaults to `false`.\n\n#### maxRecursiveDepth\nPositive number, maximum allowed depth for hyperlinks. Other dependencies will be saved regardless of their depth. Defaults to `null` - no maximum recursive depth set. \n\n#### maxDepth\nPositive number, maximum allowed depth for all dependencies. Defaults to `null` - no maximum depth set. \n\n#### request\nObject, custom options for [request](https://github.com/request/request#requestoptions-callback). Allows to set cookies, userAgent, etc.\n```javascript\nscrape({\n  urls: ['http://example.com/'],\n  directory: '/path/to/save',\n  request: {\n    headers: {\n      'User-Agent': 'Mozilla/5.0 (Linux; Android 4.2.1; en-us; Nexus 4 Build/JOP40D) AppleWebKit/535.19 (KHTML, like Gecko) Chrome/18.0.1025.166 Mobile Safari/535.19'\n    }\n  }\n}).then(console.log).catch(console.log);\n```\n\n#### subdirectories\nArray of objects, specifies subdirectories for file extensions. If `null` all files will be saved to `directory`.\n```javascript\n/* Separate files into directories:\n  - `img` for .jpg, .png, .svg (full path `/path/to/save/img`)\n  - `js` for .js (full path `/path/to/save/js`)\n  - `css` for .css (full path `/path/to/save/css`)\n*/\nscrape({\n  urls: ['http://example.com'],\n  directory: '/path/to/save',\n  subdirectories: [\n    {directory: 'img', extensions: ['.jpg', '.png', '.svg']},\n    {directory: 'js', extensions: ['.js']},\n    {directory: 'css', extensions: ['.css']}\n  ]\n}).then(console.log).catch(console.log);\n```\n\n#### defaultFilename\nString, filename for index page. Defaults to `index.html`.\n\n#### prettifyUrls\nBoolean, whether urls should be 'prettified', by having the `defaultFilename` removed. Defaults to `false`.\n\n#### ignoreErrors\nBoolean, if `true` scraper will continue downloading resources after error occured, if `false` - scraper will finish process and return error. Defaults to `true`.\n\n#### urlFilter\nFunction which is called for each url to check whether it should be scraped. Defaults to `null` - no url filter will be applied.\n```javascript\n// Links to other websites are filtered out by the urlFilter\nvar scrape = require('website-scraper');\nscrape({\n  urls: ['http://example.com/'],\n  urlFilter: function(url){\n    return url.indexOf('http://example.com') === 0;\n  },\n  directory: '/path/to/save'\n}).then(console.log).catch(console.log);\n```\n\n#### filenameGenerator\nString, name of one of the bundled filenameGenerators, or a custom filenameGenerator function. Filename generator determines where the scraped files are saved.\n\n###### byType (default)\nWhen the `byType` filenameGenerator is used the downloaded files are saved by type (as defined by the `subdirectories` setting) or directly in the `directory` folder, if no subdirectory is specified for the specific type.\n\n###### bySiteStructure\nWhen the `bySiteStructure` filenameGenerator is used the downloaded files are saved in `directory` using same structure as on the website:\n- `/` => `DIRECTORY/index.html`\n- `/about` => `DIRECTORY/about/index.html`\n- `/resources/javascript/libraries/jquery.min.js` => `DIRECTORY/resources/javascript/libraries/jquery.min.js`\n\n```javascript\n// Downloads all the crawlable files. The files are saved in the same structure as the structure of the website\n// Links to other websites are filtered out by the urlFilter\nvar scrape = require('website-scraper');\nscrape({\n  urls: ['http://example.com/'],\n  urlFilter: function(url){ return url.indexOf('http://example.com') === 0; },\n  recursive: true,\n  maxDepth: 100,\n  filenameGenerator: 'bySiteStructure',\n  directory: '/path/to/save'\n}).then(console.log).catch(console.log);\n```\n\n#### httpResponseHandler\nFunction which is called on each response, allows to customize resource or reject its downloading.\nIt takes 1 argument - response object of [request](https://github.com/request/request) module and should return resolved `Promise` if resource should be downloaded or rejected with Error `Promise` if it should be skipped.\nPromise should be resolved with:\n* `string` which contains response body\n* or object with properies `body` (response body, string) and `metadata` - everything you want to save for this resource (like headers, original text, timestamps, etc.), scraper will not use this field at all, it is only for result.\n```javascript\n// Rejecting resources with 404 status and adding metadata to other resources\nscrape({\n  urls: ['http://example.com/'],\n  directory: '/path/to/save',\n  httpResponseHandler: (response) => {\n  \tif (response.statusCode === 404) {\n\t\treturn Promise.reject(new Error('status is 404'));\n\t} else {\n\t\t// if you don't need metadata - you can just return Promise.resolve(response.body)\n\t\treturn Promise.resolve({\n\t\t\tbody: response.body,\n\t\t\tmetadata: {\n\t\t\t\theaders: response.headers,\n\t\t\t\tsomeOtherData: [ 1, 2, 3 ]\n\t\t\t}\n\t\t});\n\t}\n  }\n}).then(console.log).catch(console.log);\n```\nScrape function resolves with array of [Resource](https://github.com/s0ph1e/node-website-scraper/blob/master/lib/resource.js) objects which contain `metadata` property from `httpResponseHandler`. \n\n#### resourceSaver\nClass which saves [Resources](https://github.com/s0ph1e/node-website-scraper/blob/master/lib/resource.js), should have methods `saveResource` and `errorCleanup` which return Promises. Use it to save files where you need: to dropbox, amazon S3, existing directory, etc. By default all files are saved in local file system to new directory passed in `directory` option (see [lib/resource-saver/index.js](https://github.com/s0ph1e/node-website-scraper/blob/master/lib/resource-saver/index.js)).\n```javascript\nscrape({\n  urls: ['http://example.com/'],\n  directory: '/path/to/save',\n  resourceSaver: class MyResourceSaver {\n  \tsaveResource (resource) {/* code to save file where you need */}\n  \terrorCleanup (err) {/* code to remove all previously saved files in case of error */}\n  }\n}).then(console.log).catch(console.log);\n```\n\n#### onResourceSaved\nFunction called each time when resource is saved to file system. Callback is called with [Resource](https://github.com/s0ph1e/node-website-scraper/blob/master/lib/resource.js) object. Defaults to `null` - no callback will be called.\n```javascript\nscrape({\n  urls: ['http://example.com/'],\n  directory: '/path/to/save',\n  onResourceSaved: (resource) => {\n  \tconsole.log(`Resource ${resource} was saved to fs`);\n  }\n})\n```\n\n#### onResourceError\nFunction called each time when resource's downloading/handling/saving to fs was failed. Callback is called with - [Resource](https://github.com/s0ph1e/node-website-scraper/blob/master/lib/resource.js) object and `Error` object. Defaults to `null` - no callback will be called.\n```javascript\nscrape({\n  urls: ['http://example.com/'],\n  directory: '/path/to/save',\n  onResourceError: (resource, err) => {\n  \tconsole.log(`Resource ${resource} was not saved because of ${err}`);\n  }\n})\n```\n\n## callback \nCallback function, optional, includes following parameters:\n  - `error`: if error - `Error` object, if success - `null`\n  - `result`: if error - `null`, if success - array of [Resource](https://github.com/s0ph1e/node-website-scraper/blob/master/lib/resource.js) objects containing:\n    - `url`: url of loaded page\n    - `filename`: filename where page was saved (relative to `directory`)\n    - `children`: array of children Resources\n\n## Log and debug\nThis module uses [debug](https://github.com/visionmedia/debug) to log events. To enable logs you should use environment variable `DEBUG`.\nNext command will log everything from website-scraper\n```bash\nexport DEBUG=website-scraper*; node app.js\n```\n\nModule has different loggers for levels: `website-scraper:error`, `website-scraper:warn`, `website-scraper:info`, `website-scraper:debug`, `website-scraper:log`. Please read [debug](https://github.com/visionmedia/debug) documentation to find how to include/exclude specific loggers.\n",
  "readmeFilename": "README.md",
  "repository": {
    "type": "git",
    "url": "git://github.com/s0ph1e/node-website-scraper.git"
  },
  "scripts": {
    "eslint": "eslint lib/** index.js",
    "test": "istanbul cover node_modules/mocha/bin/_mocha --dir ./coverage --report lcov -- -R spec --recursive --timeout 7000 ./test/unit/ ./test/functional && npm run eslint",
    "test-e2e": "node_modules/mocha/bin/_mocha --timeout 300000 ./test/e2e/*-test.js"
  },
  "version": "2.4.0"
}
